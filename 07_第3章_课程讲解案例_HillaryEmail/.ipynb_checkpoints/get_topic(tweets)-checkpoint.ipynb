{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "        LDA Practise\n",
    "        \n",
    "        使用示例：\n",
    "            # 已知的推特\n",
    "            known_tweets = \\\n",
    "            '''\n",
    "            first tweet balabala ...\n",
    "    \n",
    "            second tweet balabala ...\n",
    "        \n",
    "            third tweet balabala ...\n",
    "            \n",
    "            ...\n",
    "            '''\n",
    "            \n",
    "            # 待分类的推特\n",
    "            test_tweets = \\\n",
    "            '''\n",
    "            Hillary is happy for the Election result!\n",
    "\n",
    "            Obama is not supporting Trump.\n",
    "            '''\n",
    "            \n",
    "            # 直接获取主题\n",
    "            lda = LDA(df, num_topics=5)\n",
    "            lda.get_topic(test_tweets)\n",
    "            # 结果示例\n",
    "            [(0, 0.8397428), (0, 0.7999622)]\n",
    "            # 结果解释：第一条推特有84.0%的概率属于第0个主题；第二条推特也有80%的概率属于第0个主题\n",
    "\"\"\"\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(text):\n",
    "    text = text.replace('\\n',\" \") \n",
    "    text = re.sub(r\"\\.+\", \" \", text)  # 省略号 (如 watching...never 不能成为 watchingnever)\n",
    "    text = re.sub(r\"-\", \" \", text) \n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) \n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) \n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) \n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text)\n",
    "    pure_text = ''\n",
    "    for letter in text:\n",
    "        if letter.isalpha() or letter==' ':\n",
    "            pure_text += letter\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text\n",
    "\n",
    "class LDA:\n",
    "    stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', \n",
    "                'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', \n",
    "                'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', \n",
    "                'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', \n",
    "                'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', \n",
    "                'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', \n",
    "                'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', \n",
    "                'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', \n",
    "                'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', \n",
    "                'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', \n",
    "                'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', \n",
    "                'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which']\n",
    "\n",
    "    def __init__(self, df, num_topics=1):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        df: 文件对象或文件内容字符串\n",
    "        num_topics: 目标主题数量\n",
    "        \"\"\"\n",
    "        if not isinstance(df, io.IOBase):  \n",
    "            if isinstance(df, str):\n",
    "                df = df.split(\"\\n\")\n",
    "            else:\n",
    "                return\n",
    "        \n",
    "        corpus, dictionary = self.get_corpus(df)\n",
    "\n",
    "        self.lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "    def get_topic(self, tweets):\n",
    "        \"\"\"\n",
    "        获取主题概率列表\n",
    "        \"\"\"\n",
    "        if not isinstance(tweets, io.IOBase):  \n",
    "            if isinstance(tweets, str):\n",
    "                tweets = tweets.split(\"\\n\")\n",
    "            else:\n",
    "                return\n",
    "            \n",
    "        corpus, dictionary = self.get_corpus(tweets)\n",
    "        \n",
    "        topics = [{t[0]: t[1] for t in self.lda.get_document_topics(bow)} for bow in corpus]\n",
    "        print(topics)\n",
    "        closest_topics = []\n",
    "        for topic_items in topics:\n",
    "            posibilities = [psb for psb in topic_items.values()]\n",
    "            max_psb_index = posibilities.index(max(posibilities))\n",
    "            closest_topic = (max_psb_index, topic_items[max_psb_index])\n",
    "            closest_topics.append(closest_topic)\n",
    "        return closest_topics\n",
    "#         return list(self.lda.get_document_topics(corpus))\n",
    "\n",
    "    def get_corpus(self, df):\n",
    "        # 由输入生成文档列表（这里每一个文档就是一条推特）\n",
    "        docs = [clean_tweet_text(line) for line in df if line]\n",
    "        doclist = np.array(docs)\n",
    "        \n",
    "        # 由文档列表生成词袋\n",
    "        texts = [[word for word in doc.lower().split() if word not in LDA.stoplist] for doc in doclist]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = open(\"./input/HillaryTweets.txt\")\n",
    "df = \\\n",
    "\"\"\"\n",
    "To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.\n",
    "\n",
    "I was greeted by this heartwarming display on the corner of my street today. Thank you to all of you who did this. Happy Thanksgiving. -H\n",
    "\n",
    "Hoping everyone has a safe & Happy Thanksgiving today, & quality time with family & friends. -H\n",
    "\n",
    "Scripture tells us: Let us not grow weary in doing good, for in due season, we shall reap, if we do not lose heart.\n",
    "\n",
    "Let us have faith in each other. Let us not grow weary. Let us not lose heart. For there are more seasons to come and...more work to do\n",
    "\n",
    "We have still have not shattered that highest and hardest glass ceiling. But some day, someone will\n",
    "\n",
    "To Barack and Michelle Obama, our country owes you an enormous debt of gratitude. We thank you for your graceful, determined leadership\n",
    "\n",
    "Our constitutional democracy demands our participation, not just every four years, but all the time\n",
    "\n",
    "You represent the best of America, and being your candidate has been one of the greatest honors of my life\n",
    "\n",
    "Last night I congratulated Donald Trump and offered to work with him on behalf of our country\n",
    "\n",
    "Already voted? That's great! Now help Hillary win by signing up to make calls now\n",
    "\n",
    "It's Election Day! Millions of Americans have cast their votes for Hillary—join them and confirm where you vote\n",
    "\n",
    "We don’t want to shrink the vision of this country. We want to keep expanding it\n",
    "\n",
    "We have a chance to elect a 45th president who will build on our progress, who will finish the job\n",
    "\n",
    "I love our country, and I believe in our people, and I will never, ever quit on you. No matter what\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 0.040005475, 1: 0.040008996, 2: 0.83909035, 3: 0.040886905, 4: 0.040008295}, {0: 0.050005645, 1: 0.050009314, 2: 0.79996574, 3: 0.050010726, 4: 0.050008588}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.83909035), (2, 0.79996574)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets = \\\n",
    "\"\"\"\n",
    "Hillary is happy for the Election result!\n",
    "\n",
    "Obama is not supporting Trump.\n",
    "\"\"\"\n",
    "LDA(df, num_topics=5).get_topic(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
